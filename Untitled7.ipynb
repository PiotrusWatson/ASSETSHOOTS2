{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PiotrusWatson/ASSETSHOOTS2/blob/master/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMOOZyQ1xwfK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "d778bbd9-f6d8-4fd2-b425-a8603b3431a6"
      },
      "source": [
        "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
        "!unzip snli_1.0.zip"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-17 03:53:11--  https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94550081 (90M) [application/zip]\n",
            "Saving to: ‘snli_1.0.zip.3’\n",
            "\n",
            "snli_1.0.zip.3      100%[===================>]  90.17M  18.2MB/s    in 8.2s    \n",
            "\n",
            "2019-11-17 03:53:20 (10.9 MB/s) - ‘snli_1.0.zip.3’ saved [94550081/94550081]\n",
            "\n",
            "Archive:  snli_1.0.zip\n",
            "replace snli_1.0/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK7YRq7oPCWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the Glove.zip file and expand it.\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhlX3fSGx2aQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch,keras\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data_utils\n",
        "from torch import nn\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRr_88NFOoTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataframe = pd.read_json('./snli_1.0/snli_1.0_train.jsonl', lines=True)\n",
        "test_dataframe = pd.read_json('./snli_1.0/snli_1.0_test.jsonl', lines=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEgm0N3nRAbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_to_lists(names_to_lists):\n",
        "  for key in names_to_lists:\n",
        "    names_to_lists[key] = names_to_lists[key].tolist()\n",
        "  return names_to_lists\n",
        "\n",
        "class Tokeniser:\n",
        "  def __init__(self, texts, vocab_size, max_len):\n",
        "    self.t = Tokenizer()\n",
        "    self.max_len = max_len\n",
        "    self.t.num_words = vocab_size\n",
        "    \n",
        "    full_corpus = []\n",
        "\n",
        "    for index in texts:\n",
        "      for text in texts[index]:\n",
        "        full_corpus.append(text)\n",
        "    \n",
        "    self.t.fit_on_texts(full_corpus)\n",
        "\n",
        "  def full_process(self, text):\n",
        "    \"\"\"OK SO: converts a list of strings into a list of numerical sequences\n",
        "then pads them out so they're all a consistent size\n",
        "then returns a numpy array of that :) \"\"\"\n",
        "    new_sequence = self.t.texts_to_sequences(text)\n",
        "    padded_sequence = pad_sequences(new_sequence, maxlen=self.max_len, padding =\"post\")\n",
        "    return np.array(padded_sequence, dtype=np.float32)\n",
        "\n",
        "  def do_everything(self, texts):\n",
        "    for index in texts:\n",
        "      texts[index] = self.full_process(texts[index])\n",
        "    self.word_to_id = self.t.word_index\n",
        "    return texts\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "# Get the embedding matrix using Glove. \n",
        "vocab,word2idx = None,{}\n",
        "\n",
        "def load_glove_embeddings(path, word2idx, embedding_dim):\n",
        "    \"\"\"Loading the glove embeddings\"\"\"\n",
        "    vocab_size = len(word2idx) + 1\n",
        "    print(vocab_size)\n",
        "    with open(path) as f:\n",
        "        embeddings = np.zeros((vocab_size, embedding_dim))\n",
        "        for line in f.readlines():\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            index = word2idx.get(word)\n",
        "            if index:\n",
        "                vector = np.array(values[1:], dtype='float32')\n",
        "                if vector.shape[-1] != embedding_dim:\n",
        "                    raise Exception('Dimension not matching.')\n",
        "                embeddings[index] = vector\n",
        "        return torch.from_numpy(embeddings).float()\n",
        "\n",
        "#assumption: we're going to only care about classification per text\n",
        "def generate_indexes(labels):\n",
        "  return [1 if label == \"neutral\" else 2 if label == \"entailment\" else 0 for label in labels]\n",
        "\n",
        "index_to_label = [\"contradiction\",\"neutral\",\"entailment\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INou3VKpN7_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluation_summary(description, predictions, true_labels):\n",
        "  print(\"Evaluation for: \" + description)\n",
        "  precision = precision_score(predictions, true_labels, average='macro')\n",
        "  recall = recall_score(predictions, true_labels, average='macro')\n",
        "  accuracy = accuracy_score(predictions, true_labels)\n",
        "  f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
        "  print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f\" % (description,accuracy,precision,recall,f1))\n",
        "  print(classification_report(predictions, true_labels, digits=3))\n",
        "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions))\n",
        "  return precision,recall,accuracy,f1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEFVhCmRUM2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 75\n",
        "VOCAB_SIZE = 20000\n",
        "BATCH_SIZE = 128\n",
        "SAMPLE_SAMPLE_SIZE = 4\n",
        "\n",
        "chopped_train_dataframe = train_dataframe.sample(n=int(len(train_dataframe[\"sentence1\"])/SAMPLE_SAMPLE_SIZE))\n",
        "x_train_lists = convert_to_lists({\"premise\": chopped_train_dataframe[\"sentence1\"], \"hypothesis\": chopped_train_dataframe[\"sentence2\"]})\n",
        "y_train_list = chopped_train_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "x_test_lists = convert_to_lists({\"premise\": test_dataframe[\"sentence1\"], \"hypothesis\": test_dataframe[\"sentence2\"]})\n",
        "y_test_list = test_dataframe[\"gold_label\"].tolist()\n",
        "\n",
        "\n",
        "x_tokeniser = Tokeniser(x_train_lists, VOCAB_SIZE, MAX_LENGTH)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6mbOCXki_Mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_tokeniser.do_everything(x_train_lists)\n",
        "x_test = x_tokeniser.do_everything(x_test_lists)\n",
        "y_train = np.array(generate_indexes(y_train_list), dtype=np.float32)\n",
        "y_test = np.array(generate_indexes(y_test_list), dtype=np.float32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTxMpyqkR7KV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#alright lets tensordataset this boy\n",
        "train_data = data_utils.TensorDataset(torch.from_numpy(x_train[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_train[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_train).type(torch.LongTensor))\n",
        "\n",
        "train_loader = data_utils.DataLoader(train_data, batch_size=BATCH_SIZE, drop_last=True)\n",
        "test_data = data_utils.TensorDataset(torch.from_numpy(x_test[\"premise\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(x_test[\"hypothesis\"]).type(torch.LongTensor),\n",
        "                                      torch.from_numpy(y_test).type(torch.LongTensor))\n",
        "test_loader = data_utils.DataLoader(test_data, batch_size=BATCH_SIZE, drop_last=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JwnWeWhORhm",
        "colab_type": "text"
      },
      "source": [
        "STUFF I STILL DON'T KNOW A THING ABOUT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izag5KDB6c91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "    def __init__(self, features, epsilon=1e-8):\n",
        "        '''Applies layer normalization. USE LATER\n",
        "        Args:\n",
        "          epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n",
        "        '''\n",
        "        super(layer_normalization, self).__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = nn.Parameter(torch.ones(features))\n",
        "        self.beta = nn.Parameter(torch.zeros(features))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.gamma * (x - mean) / (std + self.epsilon) + self.beta\n",
        "\n",
        "#todo: rewrite, understand, use\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9uBrXogONRG",
        "colab_type": "text"
      },
      "source": [
        "wrapper for optimiser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLyvPvF5eQ4Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
        "        \n",
        "def get_std_opt(model):\n",
        "    return NoamOpt(model.hp.attention_size, 2, 4000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw2nohf4OK0-",
        "colab_type": "text"
      },
      "source": [
        "basic feedforward network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwSVpWMlJo5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Forwarder(nn.Module):\n",
        "  #incredibly basic feedforward network\n",
        "  #two linear changes\n",
        "  def __init__(self, in_channels, num_units=[2048, 512], dropout=0.1):\n",
        "\n",
        "    super(Forwarder, self).__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.num_units = num_units\n",
        "\n",
        "    self.conv1 = nn.Sequential(nn.Linear(self.in_channels, self.num_units[0]), nn.ReLU())\n",
        "    self.conv2 = nn.Linear(self.num_units[0], self.num_units[1])\n",
        "    self.norm1 = nn.LayerNorm(self.in_channels)\n",
        "\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    \n",
        "    conv1_outputs = self.conv1(inputs)\n",
        "    conv2_outputs = self.conv2(conv1_outputs)\n",
        "\n",
        "    conv2_outputs += inputs\n",
        "    \n",
        "    outputs = self.norm1(conv2_outputs)\n",
        "\n",
        "    #normalise here\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwBQL9x_OHpG",
        "colab_type": "text"
      },
      "source": [
        "hyperparams: set them HERE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPSVL_bwIyjf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ed435af-0997-4291-9642-b05e533713d5"
      },
      "source": [
        "class Hyperparameters:\n",
        "  attention_size = 300 #hidden/num_units\n",
        "  num_heads = 5\n",
        "  dropout = 0.1\n",
        "  num_classes = 3\n",
        "  num_blocks = 6\n",
        "  epochs = 12\n",
        "  sinusoid = True\n",
        "\n",
        "glove_embeddings = load_glove_embeddings(\"glove.6B.{}d.txt\".format(Hyperparameters.attention_size),x_tokeniser.word_to_id,Hyperparameters.attention_size)"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22329\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fha2mYJWOEA2",
        "colab_type": "text"
      },
      "source": [
        "putting everything together, here's an attempt at a basic entailment model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iaiiUOKA2q-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicEntailmentModel(nn.Module):\n",
        "  def __init__(self,\n",
        "               max_length,\n",
        "               batch_size, \n",
        "               word_embeddings,\n",
        "               vocab_size,\n",
        "               hp):\n",
        "    super(BasicEntailmentModel, self).__init__()\n",
        "    self.hp = hp\n",
        "    self.premise_embeddings = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=hp.attention_size)\n",
        "    self.premise_embeddings.weight = nn.Parameter(word_embeddings)\n",
        "    self.premise_norm = nn.LayerNorm(hp.attention_size)\n",
        "\n",
        "    self.premise_positional_encoding = PositionalEncoding(hp.attention_size,\n",
        "                                                          hp.dropout)\n",
        "\n",
        "    self.hypothesis_embeddings = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=hp.attention_size)\n",
        "    self.hypothesis_embeddings.weight = nn.Parameter(word_embeddings)\n",
        "    self.hypothesis_dropout = nn.Dropout(hp.dropout)\n",
        "\n",
        "    self.hypothesis_positional_encoding = PositionalEncoding(hp.attention_size,\n",
        "                                                             hp.dropout)\n",
        "    self.hypothesis_norm = nn.LayerNorm(hp.attention_size)\n",
        "    self.word_embeddings_size = word_embeddings.size(1)\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "\n",
        "    \n",
        "    #create a buncha attributes with THIS HACKY CODE -> attention into feedforward\n",
        "    for i in range(self.hp.num_blocks):\n",
        "      self.__setattr__('premise_self_attention%d' % i, nn.MultiheadAttention(hp.attention_size,\n",
        "                                                                           num_heads=hp.num_heads,\n",
        "                                                                           dropout=hp.dropout))\n",
        "      self.__setattr__('premise_feed_forward%d' % i, Forwarder(in_channels=hp.attention_size,\n",
        "                                                               num_units = [4*hp.attention_size,\n",
        "                                                                            hp.attention_size]))\n",
        "    \n",
        "    #same for hypothesis\n",
        "    for i in range(self.hp.num_blocks):\n",
        "      self.__setattr__('hypothesis_self_attention%d' % i, nn.MultiheadAttention(hp.attention_size,\n",
        "                                                                           num_heads=hp.num_heads,\n",
        "                                                                           dropout=hp.dropout))\n",
        "      self.__setattr__('hypothesis_feed_forward%d' % i, Forwarder(in_channels=hp.attention_size,\n",
        "                                                               num_units = [4*hp.attention_size,\n",
        "                                                                           hp.attention_size]))\n",
        "      \n",
        "    self.mlp1 = nn.Linear(hp.attention_size * max_length, 20)\n",
        "    self.mlp2 = nn.Linear(20, hp.num_classes)\n",
        "    \n",
        "  def forward(self, premise_words, hypothesis_words):\n",
        "    self.premise = self.premise_embeddings(premise_words) * math.sqrt(self.hp.attention_size)\n",
        "    self.premise = self.premise_positional_encoding(self.premise) \n",
        "    \n",
        "    \n",
        "\n",
        "    for i in range(self.hp.num_blocks):\n",
        "      self.premise, _ = self.__getattr__('premise_self_attention%d' % i)(self.premise, \n",
        "                                                                      self.premise, \n",
        "                                                                      self.premise)\n",
        "      self.premise = self.__getattr__('premise_feed_forward%d' % i)(self.premise)\n",
        "\n",
        "    self.premise = self.premise_norm(self.premise)\n",
        "    self.hypothesis = self.hypothesis_embeddings(hypothesis_words)* math.sqrt(self.hp.attention_size)\n",
        "    self.hypothesis = self.hypothesis_positional_encoding(self.hypothesis)\n",
        "\n",
        "    for i in range(self.hp.num_blocks):\n",
        "      self.hypothesis, _ = self.__getattr__('hypothesis_self_attention%d' % i)(self.hypothesis, \n",
        "                                                                      self.hypothesis, \n",
        "                                                                      self.hypothesis)\n",
        "      self.hypothesis = self.__getattr__('hypothesis_feed_forward%d' % i)(self.hypothesis)\n",
        "\n",
        "    self.hypothesis = self.hypothesis_norm(self.hypothesis)\n",
        "    \n",
        "    self.mush = self.premise * self.hypothesis\n",
        "    self.mush = self.mush.reshape(self.batch_size, -1)\n",
        "    self.mush = self.mlp1(self.mush)\n",
        "    self.output = torch.softmax(self.mlp2(self.mush),-1)\n",
        "    return self.output\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6NQnWUvOCVM",
        "colab_type": "text"
      },
      "source": [
        "training function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6SVXr1EF6IG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model=None, \n",
        "          train_loader=None, \n",
        "          loss_function=None, \n",
        "          optimiser=None, \n",
        "          epochs=5, \n",
        "          using_gradient_clipping=False):\n",
        "  \n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    print(\"Running EPOCH:\",epoch+1)\n",
        "    total_loss = 0\n",
        "    batch_count = 0\n",
        "    correct = 0\n",
        "\n",
        "    for batch_index, train_data in enumerate(train_loader):\n",
        "      premise = Variable(train_data[0]).cuda()\n",
        "      hypothesis = Variable(train_data[1]).cuda()\n",
        "      actual_y = Variable(train_data[2]).cuda()\n",
        "      \n",
        "      torch.cuda.synchronize()\n",
        "      optimiser.optimizer.zero_grad()\n",
        "      predicted_y = model(premise, hypothesis)\n",
        "      squeezed_y = predicted_y.double().squeeze(1)\n",
        "      loss = loss_function(squeezed_y,actual_y.long())\n",
        "\n",
        "      correct += torch.eq(torch.argmax(squeezed_y, 1), actual_y).data.sum()\n",
        "      \n",
        "      if (batch_index == 1):\n",
        "        print(total_loss, correct.data.cpu().numpy().astype(int)/train_loader.batch_size)\n",
        "      total_loss += loss.data\n",
        "\n",
        "      #woah we gotta do this to do backprop!!!\n",
        "      optimiser.optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      torch.cuda.synchronize()\n",
        "\n",
        "      if using_gradient_clipping:\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
        "      batch_count += 1\n",
        "      optimiser.step()\n",
        "\n",
        "    print(\"Average loss is:\",total_loss/batch_count)\n",
        "    correct_but_numpy = correct.data.cpu().numpy().astype(int)\n",
        "    accuracy = correct_but_numpy / float(batch_count * train_loader.batch_size)\n",
        "    print(\"Accuracy of the model\", accuracy)\n",
        "    losses.append(total_loss/batch_count)\n",
        "    accuracies.append(accuracy)\n",
        "  return losses, accuracies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC95R5iyOc4u",
        "colab_type": "text"
      },
      "source": [
        "test function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25tdNU7LObr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, test_premise, test_hypothesis, test_y):\n",
        "\n",
        "  model.batch_size = test_hypothesis.shape[0] #why\n",
        "  model = model.cpu()\n",
        "  premise_variable = Variable(torch.from_numpy(test_premise).type(torch.LongTensor))\n",
        "  hypothesis_variable = Variable(torch.from_numpy(test_hypothesis).type(torch.LongTensor))\n",
        "  y_actual_variable = Variable(torch.from_numpy(test_y).type(torch.DoubleTensor))\n",
        "  y_predicted = model(premise_variable, hypothesis_variable)\n",
        "  y_predicted_rounded = torch.round(y_predicted.type(torch.DoubleTensor).squeeze(1))\n",
        "\n",
        "  test_data_count = premise_variable.size(0)\n",
        "\n",
        "\n",
        "\n",
        "  total_accuracy = torch.eq(torch.argmax(y_predicted,1), y_actual_variable).data.sum()\n",
        "  average_accuracy = total_accuracy.data.cpu().numpy().astype(int)/float(test_data_count)\n",
        "  return average_accuracy, torch.argmax(y_predicted,1)\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tueScWvqOpCF",
        "colab_type": "text"
      },
      "source": [
        "ok lets run\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d88Jpa8Op7y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "bc079a17-cff9-4846-ac0a-d2e3d7f18f88"
      },
      "source": [
        "model = BasicEntailmentModel(max_length = MAX_LENGTH,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             vocab_size=VOCAB_SIZE,\n",
        "                             word_embeddings=glove_embeddings,\n",
        "                             hp=Hyperparameters\n",
        "                             )\n",
        "model = model.cuda()\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "optimiser = get_std_opt(model)\n",
        "loss, accuracy = train(model=model,\n",
        "                       train_loader=train_loader,\n",
        "                       loss_function=loss,\n",
        "                       optimiser = optimiser,\n",
        "                       epochs = 5,\n",
        "                       using_gradient_clipping=True\n",
        "                      )"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running EPOCH: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(1.0944, device='cuda:0', dtype=torch.float64) 0.734375\n",
            "Average loss is: tensor(1.1020, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.33554469273743015\n",
            "Running EPOCH: 2\n",
            "tensor(1.1015, device='cuda:0', dtype=torch.float64) 0.625\n",
            "Average loss is: tensor(1.0993, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.3349409334264432\n",
            "Running EPOCH: 3\n",
            "tensor(1.1029, device='cuda:0', dtype=torch.float64) 0.625\n",
            "Average loss is: tensor(1.0989, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.3356392574487896\n",
            "Running EPOCH: 4\n",
            "tensor(1.1002, device='cuda:0', dtype=torch.float64) 0.6484375\n",
            "Average loss is: tensor(1.0988, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.3348099976722533\n",
            "Running EPOCH: 5\n",
            "tensor(1.0989, device='cuda:0', dtype=torch.float64) 0.625\n",
            "Average loss is: tensor(1.0987, device='cuda:0', dtype=torch.float64)\n",
            "Accuracy of the model 0.33294780027932963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSrYQ3HOO16c",
        "colab_type": "text"
      },
      "source": [
        "testan\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7pXaM-aO3PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy, predicted_ys = evaluate(model, \n",
        "         test_hypothesis=x_test[\"hypothesis\"],\n",
        "         test_premise=x_test[\"premise\"],\n",
        "         test_y=y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmx1wl0rTzCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}